<img width="1918" height="1027" alt="response" src="https://github.com/user-attachments/assets/bb17c013-1f28-47dd-8acd-e444adc4c46d" />
---

# ğŸš€ **RAG Chatbot â€” AI Document Question Answering System**

An intelligent **Retrieval-Augmented Generation (RAG) chatbot** that allows users to upload documents and ask questions based on their content.
The system retrieves relevant context using vector search and generates accurate answers using an LLM.

This project demonstrates **LLM integration, vector databases, semantic search, and full-stack AI deployment**.

---

# ğŸ“Œ **Project Overview**

This system allows users to:

âœ… Upload PDF or text documents
âœ… Automatically index documents using embeddings
âœ… Perform semantic search using FAISS vector database
âœ… Generate context-aware answers using LLM
âœ… Chat with documents in real time
âœ… Store chat history in browser
âœ… Modern ChatGPT-style frontend UI

---

# ğŸ—ï¸ **Architecture**

```
User â†’ Frontend UI â†’ FastAPI Backend â†’ FAISS Vector Search â†’ LLM Response
```

### Workflow:

1. User uploads documents
2. Documents are split into chunks
3. Text converted to embeddings
4. Stored in FAISS vector database
5. User asks question
6. Relevant chunks retrieved
7. LLM generates answer from context

---

# ğŸ–¼ï¸ **Screenshots**

## ğŸ“Š System Overview
<img width="1910" height="1031" alt="overview" src="https://github.com/user-attachments/assets/1418d113-5583-4a91-a05e-5ad7c4446e8c" />

<img src="overview.png" width="900">

---

## ğŸ’¬ Chatbot Response Interface
<img width="1918" height="1027" alt="response" src="https://github.com/user-attachments/assets/e6d369e6-fdf4-4cc7-ab4d-25051eb498f5" />


<img src="response.png" width="900">

---

# âš¡ **Features**

* ğŸ“„ PDF & TXT document upload
* ğŸ” Semantic search using FAISS
* ğŸ¤– LLM-powered answers (Groq LLaMA model)
* ğŸ’¾ Persistent vector storage
* ğŸ§  Sentence transformer embeddings
* ğŸ¨ ChatGPT-like frontend
* ğŸ“‚ Drag & drop file upload
* ğŸ™ï¸ Voice input support (frontend)
* ğŸ’¬ Streaming typing effect
* ğŸ“œ Chat history stored in browser

---

# ğŸ§° **Tech Stack**

## Backend

* FastAPI
* LangChain
* FAISS Vector Store
* Sentence Transformers
* Groq LLM API
* Python

## Frontend

* HTML
* CSS
* JavaScript
* Markdown rendering

## AI / ML

* all-MiniLM-L6-v2 Embedding Model
* Retrieval Augmented Generation (RAG)

---

# ğŸ“ **Project Structure**

```
chatbot/
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ style.css
â”‚   â””â”€â”€ script.js
â”‚
â”œâ”€â”€ faiss_store/
â”‚   â”œâ”€â”€ index.faiss
â”‚   â””â”€â”€ metadata.pkl
â”‚
â”œâ”€â”€ uploads/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ data_loader.py
â”œâ”€â”€ embedding.py
â”œâ”€â”€ search.py
â”œâ”€â”€ vectorstore.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

# âš™ï¸ **Installation & Setup**

## 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/yourusername/rag-chatbot.git
cd rag-chatbot
```

---

## 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
venv\Scripts\activate
```

---

## 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

## 4ï¸âƒ£ Setup Environment Variables

Create `.env` file:

```
GROQ_API_KEY=your_api_key_here
```

---

## 5ï¸âƒ£ Run Backend Server

```bash
uvicorn app:app --reload
```

Server runs at:

```
http://localhost:8000
```

---

## 6ï¸âƒ£ Open Frontend

Open:

```
frontend/index.html
```

---

# ğŸ§  **How RAG Works in This Project**

### Step 1 â€” Document Processing

* Load PDF/TXT files
* Split into chunks

### Step 2 â€” Embedding Generation

* Convert text â†’ vector embeddings

### Step 3 â€” Vector Storage

* Store embeddings in FAISS index

### Step 4 â€” Query Processing

* Convert query â†’ embedding
* Retrieve top matching chunks

### Step 5 â€” LLM Response

* Context passed to LLM
* Generates final answer

---

# ğŸ” **Environment Variables**

| Variable     | Description          |
| ------------ | -------------------- |
| GROQ_API_KEY | API key for Groq LLM |

---

# ğŸ“ˆ **Future Improvements**

* Authentication system
* Multi-user chat sessions
* Streaming backend responses
* Docker deployment
* Cloud storage for vectors
* Better source citation
* Model switching support

---

# ğŸ‘¨â€ğŸ’» **Author**

**Upayan Chatterjee**

* AI Engineer & Full Stack Developer
* Interested in LLMs, AI systems, and real-world applications

---

# â­ **If You Like This Project**

Give it a â­ on GitHub â€” it helps!

---

If you want, I can next help you make this even more **industry-level portfolio ready**, like:

âœ… animated README with badges
âœ… GitHub shields (build, license, python version etc.)
âœ… architecture diagram image
âœ… deployment section
âœ… demo video section
âœ… professional project description for placements

Just tell me ğŸ‘
